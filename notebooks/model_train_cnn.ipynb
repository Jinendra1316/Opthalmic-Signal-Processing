{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9feae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, time, json, warnings, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28338465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cpu | CUDA: None | cuda_available: False\n",
      "NumPy: 1.26.4\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| cuda_available:\", torch.cuda.is_available())\n",
    "try:\n",
    "    import numpy as np\n",
    "    import numpy\n",
    "    print(\"NumPy:\", numpy.__version__)\n",
    "except Exception as e:\n",
    "    print(\"NumPy import issue:\", e)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52114be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"perg/csv\"  # <-- change path if needed\n",
    "\n",
    "NEED_DATA = \"per_eye_avg\" not in globals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99691aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] building per_eye_avg …\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'record_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_39532\\3694115423.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    110\u001b[39m             })\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(rows)\n\u001b[32m    112\u001b[39m \n\u001b[32m    113\u001b[39m     print(\u001b[33m\"[data] building per_eye_avg …\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     per_eye = load_per_eye_table(DATA_DIR)\n\u001b[32m    115\u001b[39m     per_eye_avg = average_repeats(per_eye)\n\u001b[32m    116\u001b[39m     print(\u001b[33m\"per_eye:\"\u001b[39m, per_eye.shape, \u001b[33m\"| per_eye_avg:\"\u001b[39m, per_eye_avg.shape)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_39532\\3694115423.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(data_dir)\u001b[39m\n\u001b[32m     60\u001b[39m                                      \"n_samples\": len(sig), \"fs_hz\": float(fs), \"signal\": sig})\n\u001b[32m     61\u001b[39m \n\u001b[32m     62\u001b[39m         per_eye = pd.DataFrame(rows)\n\u001b[32m     63\u001b[39m         key_col = \u001b[33m\"id_record\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"id_record\"\u001b[39m \u001b[38;5;28;01min\u001b[39;00m demo.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"record_id\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         per_eye = per_eye.merge(demo, left_on=\u001b[33m\"record_id\"\u001b[39m, right_on=key_col, how=\u001b[33m\"left\"\u001b[39m)\n\u001b[32m     65\u001b[39m \n\u001b[32m     66\u001b[39m         \u001b[38;5;66;03m# targets\u001b[39;00m\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"diagnosis1\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m per_eye.columns:\n",
      "\u001b[32md:\\perg\\venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10835\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m     ) -> DataFrame:\n\u001b[32m  10837\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10838\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m         return merge(\n\u001b[32m  10840\u001b[39m             self,\n\u001b[32m  10841\u001b[39m             right,\n\u001b[32m  10842\u001b[39m             how=how,\n",
      "\u001b[32md:\\perg\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32md:\\perg\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32md:\\perg\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1307\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1310\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1312\u001b[39m                         join_names.append(lk)\n\u001b[32m   1313\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32md:\\perg\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'record_id'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from scipy.signal import butter, filtfilt, welch\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "    def skew(x): return 0.0\n",
    "    def kurtosis(x): return 0.0\n",
    "\n",
    "if NEED_DATA:\n",
    "    def _estimate_fs(time_series):\n",
    "        try:\n",
    "            t = pd.to_datetime(time_series, format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "            dt = (t.view(\"int64\")[1:] - t.view(\"int64\")[:-1]) / 1e9\n",
    "            return float(1.0 / np.median(dt))\n",
    "        except Exception:\n",
    "            try:\n",
    "                vals = pd.to_numeric(time_series, errors=\"coerce\").to_numpy(dtype=float)\n",
    "                dt = np.diff(vals) / 1000.0\n",
    "                return float(1.0 / np.median(dt))\n",
    "            except Exception:\n",
    "                return 1700.0\n",
    "\n",
    "    def load_per_eye_table(data_dir):\n",
    "        demo_path = os.path.join(data_dir, \"D:/perg/data/raw/participants_info.csv\") \n",
    "        if not os.path.exists(demo_path):\n",
    "            raise FileNotFoundError(f\"D:/perg/data/raw/participants_info.csv not found in {data_dir}\")\n",
    "        demo = pd.read_csv(demo_path)\n",
    "        demo.columns = [c.strip() for c in demo.columns]\n",
    "\n",
    "        files = sorted(glob.glob(os.path.join(data_dir, \"[0-9][0-9][0-9][0-9].csv\")))\n",
    "        rows = []\n",
    "        time_pat = re.compile(r'^TIME_(\\d+)$')\n",
    "        re_pat   = re.compile(r'^RE_(\\d+)$')\n",
    "        le_pat   = re.compile(r'^LE_(\\d+)$')\n",
    "\n",
    "        for fp in files:\n",
    "            rec = pd.read_csv(fp)\n",
    "            rec.columns = [c.strip() for c in rec.columns]\n",
    "            rec_id = int(Path(fp).stem)\n",
    "            cols = set(rec.columns)\n",
    "\n",
    "            # discover repeats k\n",
    "            ks = set()\n",
    "            for c in cols:\n",
    "                for pat in (time_pat, re_pat, le_pat):\n",
    "                    m = pat.match(c)\n",
    "                    if m: ks.add(int(m.group(1)))\n",
    "            if not ks: ks = {1}\n",
    "\n",
    "            for k in sorted(ks):\n",
    "                time_col = f\"TIME_{k}\" if f\"TIME_{k}\" in cols else (\"TIME\" if \"TIME\" in cols else None)\n",
    "                re_col   = f\"RE_{k}\"   if f\"RE_{k}\"   in cols else (\"RE\" if \"RE\" in cols else None)\n",
    "                le_col   = f\"LE_{k}\"   if f\"LE_{k}\"   in cols else (\"LE\" if \"LE\" in cols else None)\n",
    "                fs = _estimate_fs(rec[time_col]) if time_col is not None else 1700.0\n",
    "                for eye, col in ((\"RE\", re_col), (\"LE\", le_col)):\n",
    "                    if col and col in rec.columns:\n",
    "                        sig = pd.to_numeric(rec[col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "                        rows.append({\"record_id\": rec_id, \"eye\": eye, \"repeat\": k,\n",
    "                                     \"n_samples\": len(sig), \"fs_hz\": float(fs), \"signal\": sig})\n",
    "\n",
    "        per_eye = pd.DataFrame(rows)\n",
    "        key_col = \"id_record\" if \"id_record\" in demo.columns else \"record_id\"\n",
    "        per_eye = per_eye.merge(demo, left_on=\"record_id\", right_on=key_col, how=\"left\")\n",
    "\n",
    "        # targets\n",
    "        if \"diagnosis1\" not in per_eye.columns:\n",
    "            warnings.warn(\"diagnosis1 not found; default y_class=0\")\n",
    "            per_eye[\"y_class\"] = 0\n",
    "        else:\n",
    "            per_eye[\"y_class\"] = (per_eye[\"diagnosis1\"].fillna(\"Normal\").ne(\"Normal\")).astype(int)\n",
    "\n",
    "        re_col = next((c for c in per_eye.columns if c.lower() in (\"va_re_logmar\",\"logmar_re\",\"va_re\")), None)\n",
    "        le_col = next((c for c in per_eye.columns if c.lower() in (\"va_le_logmar\",\"logmar_le\",\"va_le\")), None)\n",
    "        per_eye[\"y_reg\"] = np.where(per_eye[\"eye\"].eq(\"RE\"), per_eye.get(re_col), per_eye.get(le_col))\n",
    "        return per_eye\n",
    "\n",
    "    def baseline_and_filter(x, fs, baseline_ms=(0,10), band=(1.0,100.0)):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        if not np.isfinite(fs) or fs <= 0: fs = 1700.0\n",
    "        n0 = max(1, int(fs*baseline_ms[1]/1000.0))\n",
    "        y = x - float(np.nanmean(x[:n0]))\n",
    "        if SCIPY_OK and band is not None:\n",
    "            lo, hi = band\n",
    "            if hi >= fs/2.0: hi = fs/2.0 - 1.0\n",
    "            if hi > lo and hi > 0:\n",
    "                b,a = butter(2, [lo/(fs/2.0), hi/(fs/2.0)], btype=\"band\")\n",
    "                y = filtfilt(b,a,y,method=\"gust\")\n",
    "        s = np.nanstd(y)\n",
    "        if s > 1e-8: y = (y - np.nanmean(y)) / s\n",
    "        return y\n",
    "\n",
    "    def average_repeats(per_eye):\n",
    "        rows = []\n",
    "        for (rec, eye), grp in per_eye.groupby([\"record_id\", \"eye\"]):\n",
    "            signals = []\n",
    "            for _, r in grp.iterrows():\n",
    "                fs = float(r.get(\"fs_hz\", 1700.0)) if pd.notna(r.get(\"fs_hz\")) else 1700.0\n",
    "                sig = baseline_and_filter(r[\"signal\"], fs)\n",
    "                signals.append(sig)\n",
    "            L = min(len(s) for s in signals)\n",
    "            signals = [s[:L] for s in signals]\n",
    "            avg_sig = np.mean(np.stack(signals, axis=0), axis=0)\n",
    "            rows.append({\n",
    "                \"record_id\": rec, \"eye\": eye, \"signal\": avg_sig,\n",
    "                \"fs_hz\": float(grp[\"fs_hz\"].iloc[0]) if pd.notna(grp[\"fs_hz\"].iloc[0]) else 1700.0,\n",
    "                \"y_class\": int(grp[\"y_class\"].iloc[0]),\n",
    "                \"y_reg\": float(grp[\"y_reg\"].iloc[0]) if pd.notna(grp[\"y_reg\"].iloc[0]) else np.nan,\n",
    "                \"n_repeats\": len(grp)\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    print(\"[data] building per_eye_avg …\")\n",
    "    per_eye = load_per_eye_table(DATA_DIR)\n",
    "    per_eye_avg = average_repeats(per_eye)\n",
    "    print(\"per_eye:\", per_eye.shape, \"| per_eye_avg:\", per_eye_avg.shape)\n",
    "else:\n",
    "    print(\"[data] using existing per_eye_avg in notebook.\")\n",
    "\n",
    "# expose baseline_and_filter if not defined\n",
    "if \"baseline_and_filter\" not in globals():\n",
    "    def baseline_and_filter(x, fs, baseline_ms=(0,10), band=(1.0,100.0)):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        n0 = max(1, int(fs*baseline_ms[1]/1000.0))\n",
    "        y = x - float(np.nanmean(x[:n0]))\n",
    "        s = np.nanstd(y)\n",
    "        if s > 1e-8: y = (y - np.nanmean(y)) / s\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2f120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd7ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
